{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classifier.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OmarMeriwani/CE888-github-Omar-Meriwani/blob/master/Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "nu5V0YRFWOdw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This document contains the code for the Deep neural network and the auto encoder classifier for CE888 Assignment 2, Data science."
      ]
    },
    {
      "metadata": {
        "id": "hjRyrrkOWGHp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import winsound\n",
        "import keras.initializers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from keras.layers import BatchNormalization\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import seaborn\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "np.random.seed(1337)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hSjFhacjWPpA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This parameter is used to swith the dataset that you want to work on, (diabetes, autism, heartattack)"
      ]
    },
    {
      "metadata": {
        "id": "4owW-d50WPxE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test = 'heartattack'\n",
        "tobestored = False\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B4Cn6FvaWP4S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For diabetes, only the fields are loaded with no other changes"
      ]
    },
    {
      "metadata": {
        "id": "1hL99OKbWP-9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if test == 'diabetes':\n",
        "    '''\n",
        "    ===================== DIABETES ======================'''\n",
        "    df = pd.read_csv('diabetes.csv',header=0)\n",
        "    X = df.iloc[:, 0:7].values\n",
        "    y = df.iloc[:, 8].values\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WOKCYvBYWQGa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For heart attack dataset, we have chosen the required fields, execluding the ones with null values, and then we execluded some rows that have few null values."
      ]
    },
    {
      "metadata": {
        "id": "0Yo7fP1zW4Qm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if test == 'heartattack':\n",
        "    '''\n",
        "    ===================== HEART-ATTACK ======================\n",
        "    '''\n",
        "    df = pd.read_csv('heart-attack.csv',header=0)\n",
        "    X = df.iloc[:, [0,1,2,3,4,5,6,7,8,9,13]].values\n",
        "    X = [k for k in X if '?' not in [m for m in k]]\n",
        "    X = [[float(j) for j in i] for i in X]\n",
        "    X = np.array(X)\n",
        "    df = pd.DataFrame(X)\n",
        "\n",
        "    y = X[:, 10]\n",
        "    X = X[:, [0,1,2,3,4,5,6,8,9]]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3WNfRQTNW4X3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For autism dataset, we have loaded the data and labled some of the fields and then we only chose a specific range of fields."
      ]
    },
    {
      "metadata": {
        "id": "WDo4wWw-W4fT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if test == 'autism':\n",
        "\n",
        "    '''\n",
        "    ===================== AUTISM ======================\n",
        "    Case_No,A1,A2,A3,A4,A5,A6,A7,A8,A9,A10,Age_Mons,Qchat-10-Score,Sex,Ethnicity,Jaundice,Family_mem_with_ASD,Who completed the test,\"Class/ASD Traits \"\n",
        "    1,0,0,0,0,0,0,1,1,0,1,28,3,f,middle eastern,yes,no,family member,No\n",
        "    '''\n",
        "    df = pd.read_csv('Autism.csv',header=0)\n",
        "    le = LabelEncoder()\n",
        "    df.iloc[:,18] = le.fit_transform(df.iloc[:,18])\n",
        "    df.iloc[:,17] = le.fit_transform(df.iloc[:,17])\n",
        "    df.iloc[:,16] = le.fit_transform(df.iloc[:,16])\n",
        "    df.iloc[:,15] = le.fit_transform(df.iloc[:,15])\n",
        "    df.iloc[:,14] = le.fit_transform(df.iloc[:,14])\n",
        "    df.iloc[:,13] = le.fit_transform(df.iloc[:,13])\n",
        "\n",
        "    y = df.iloc[:, 18].values\n",
        "    X = df.iloc[:, [1,2,3,4,15,14,13,11]].values\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qb5UMceUW4mX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The code to create heat map for correlation between columns"
      ]
    },
    {
      "metadata": {
        "id": "BFqtWSeQXYa1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "s=seaborn.heatmap(df.corr(),cmap='coolwarm', linewidths=.5)\n",
        "s.set_yticklabels(s.get_yticklabels(),rotation=45,fontsize=5)\n",
        "s.set_xticklabels(s.get_xticklabels(),rotation=45,fontsize=5)\n",
        "if tobestored == True:\n",
        "    plt.savefig('heatmap-'+ test +'.png')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "up5Byp2yXYiS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Some general parameters to enable or disable the "
      ]
    },
    {
      "metadata": {
        "id": "ygaC09XXXYo9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "WithAE = True\n",
        "AEdim = 5\n",
        "dim = len(X[0])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vCKg_I40XYx8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Autoencoder architecture"
      ]
    },
    {
      "metadata": {
        "id": "-5ddWQaoXr2X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if WithAE == True:\n",
        "    input_lyr = Input(shape=(dim,))\n",
        "    encoded = Dense(15, name='enc1', activation='relu')(input_lyr)\n",
        "    encoded = BatchNormalization()(encoded)\n",
        "    encoded = Dropout(rate=0.1)(encoded)\n",
        "    encoded = Dense(15, name='enc2',activation='relu')(encoded)\n",
        "    encoded = Dense(13, name='enc3',activation='relu')(encoded)\n",
        "    encoded = Dense(13, name='enc4',activation='relu')(encoded)\n",
        "    encoded = Dense(AEdim, name='enc5',activation='relu')(encoded)\n",
        "\n",
        "    decoded = Dense(AEdim, name='dec1',activation='relu')(encoded)\n",
        "    decoded = Dense(13, name='dec2',activation='relu')(decoded)\n",
        "    decoded = Dense(13, name='dec3',activation='relu')(decoded)\n",
        "    decoded = Dense(15, name='dec4',activation='relu')(decoded)\n",
        "    decoded = Dropout(rate=0.1)(decoded)\n",
        "    decoded = BatchNormalization()(decoded)\n",
        "    decoded = Dense(15, name='dec5',activation='relu')(decoded)\n",
        "    decoded = Dense(dim, name='dec6',activation='relu')(decoded)\n",
        "    AE = Model(input_lyr, decoded)\n",
        "    Encoder = Model(input_lyr,encoded)\n",
        "\n",
        "    AE.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    AE.fit(X,X,epochs=2000,batch_size=100,shuffle=True,verbose=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "emk_U5KXXr9o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The creation of two DNN models, one to work with encoded data and the other to work with original data."
      ]
    },
    {
      "metadata": {
        "id": "hR28JbB2XsEW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model2 = Sequential()\n",
        "\n",
        "sd_value = 0.01\n",
        "print(y.shape, X.shape)\n",
        "\n",
        "print(y.shape, X.shape)\n",
        "#model.add(GaussianNoise(stddev=sd_value, input_shape=(dim,)))\n",
        "#model.add(GaussianDropout(rate = 0.9))\n",
        "model.add(Dense(14, activation='relu', input_shape=(dim,)))\n",
        "model.add(Dense(2, name='dense5', activation='softmax'))\n",
        "\n",
        "#model.add(GaussianNoise(stddev=sd_value))\n",
        "#model.add(GaussianDropout(rate = 0.9))\n",
        "model2.add(Dense(14, activation='relu', input_shape=(AEdim,)))\n",
        "model2.add(Dense(2, name='dense5', activation='softmax'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FhQ77NAqXsLM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Compiling the models, creating general parameters and another instance of the input data"
      ]
    },
    {
      "metadata": {
        "id": "mMkcJZ71X6Us",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "folds = 5\n",
        "j = 0\n",
        "score= []\n",
        "score2= []\n",
        "X2 = X\n",
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
        "model2.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iH0rbr3vX6bm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Encoding the data using Encoder."
      ]
    },
    {
      "metadata": {
        "id": "Sms67oPUX6kM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if WithAE == True:\n",
        "    X2 = Encoder.predict(X)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_TK441YEX6qF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Final fit and evaluation"
      ]
    },
    {
      "metadata": {
        "id": "gANGWopFWQMr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "for k in range(0,2):\n",
        "    kf= StratifiedKFold(n_splits=folds, random_state=121, shuffle=True)\n",
        "    for train_index, test_index in kf.split(X, y):\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        X_train2, X_test2 = X2[train_index], X2[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "        y_train = keras.utils.np_utils.to_categorical(y_train)\n",
        "        y_test = keras.utils.np_utils.to_categorical(y_test)\n",
        "        model.fit(X_train,y_train,epochs=1000,batch_size=100,shuffle=False,verbose=0)\n",
        "        model2.fit(X_train2,y_train,epochs=1000,batch_size=100,shuffle=False,verbose=0)\n",
        "\n",
        "        score.append( model.evaluate(X_test, y_test))\n",
        "        score2.append( model2.evaluate(X_test2, y_test))\n",
        "\n",
        "avgacc = [acc for loss, acc in score]\n",
        "avgloss = [loss for loss, acc in score]\n",
        "\n",
        "avgacc2 = [acc for loss, acc in score2]\n",
        "avgloss2 = [loss for loss, acc in score2]\n",
        "\n",
        "avgacc =sum(avgacc) / len(avgacc)\n",
        "avgloss = sum(avgloss) / len(avgloss)\n",
        "\n",
        "avgacc2 =sum(avgacc2) / len(avgacc2)\n",
        "avgloss2 = sum(avgloss2) / len(avgloss2)\n",
        "\n",
        "print('Results', (avgacc2 * 100).__round__(2),'\\t', (avgloss2 *100).__round__(2),'\\t',(avgacc * 100).__round__(2), '\\t',(avgloss*100).__round__(2))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}